{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEORJvvUi8Ph"
   },
   "source": [
    "## importações e configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPdnRhChiz4l"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZGxxc2QAiJ8"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20183,
     "status": "ok",
     "timestamp": 1763459340404,
     "user": {
      "displayName": "Eduardo Medeiros",
      "userId": "10594247742830776987"
     },
     "user_tz": 180
    },
    "id": "skq74utyjDKQ",
    "outputId": "9f12b019-4888-4051-9f2a-29c760c254a8"
   },
   "outputs": [],
   "source": [
    "# Caminho raiz do projeto\n",
    "PROJ = Path(\"/content/drive/MyDrive/tcc-modelo/tcc-demand-forecasting\")\n",
    "\n",
    "# monta o drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Garante que o PROJECT_DIR está no sys.path\n",
    "if str(PROJ) not in sys.path:\n",
    "    sys.path.append(str(PROJ))\n",
    "\n",
    "print(\"Repositório ativo em:\", PROJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zozQ1nCbk6XM"
   },
   "outputs": [],
   "source": [
    "from src.evaluations.models_metrics import calculate_metrics, compare_models\n",
    "\n",
    "interim_dir = PROJ / \"data\" / \"interim\"\n",
    "output_name_imputed = \"olist_weekly_agg_withlags_imputed_2.parquet\"\n",
    "df_path = interim_dir / output_name_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Z8cSL-NlV6o"
   },
   "outputs": [],
   "source": [
    "# Colunas do seu dataset\n",
    "cutoff_col  = \"order_week\"   # coluna temporal (datetime)\n",
    "date_col    = \"order_week\"   # mesma coluna para carimbar previsões\n",
    "target_col  = \"sales_qty\"    # alvo\n",
    "id_col      = \"id\"   # opcional\n",
    "\n",
    "# Períodos (exemplo)\n",
    "first_train_end = pd.Timestamp(\"2018-03-18\")\n",
    "test_start      = pd.Timestamp(\"2018-03-19\")\n",
    "test_end        = pd.Timestamp(\"2018-08-27\")\n",
    "\n",
    "# Janela de rolling (ex.: blocos de 4 semanas)\n",
    "horizon_stride = (pd.Timedelta(days=4), pd.Timedelta(days=7))  # (gap após cutoff, janela)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1WhkOLMk1V8"
   },
   "source": [
    "## definição do df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vRoR_K0k7ui"
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1XVIc9vpGYU"
   },
   "outputs": [],
   "source": [
    "df['id'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1763126523238,
     "user": {
      "displayName": "Eduardo Medeiros",
      "userId": "10594247742830776987"
     },
     "user_tz": 180
    },
    "id": "Ol858dBtlqdS",
    "outputId": "ca187604-a9a9-44f4-8edd-a42b7e750e30"
   },
   "outputs": [],
   "source": [
    "deciles = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "df['order_week'].describe(percentiles=deciles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5BQWCqjjIv7"
   },
   "source": [
    "## funcoes uteis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUAhrRHsjKWT"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- 1) Esquemas de treinamento ----------\n",
    "def split_static(df, cutoff_col, first_train_end, test_start, test_end) -> List[Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"Treina uma vez até first_train_end e prevê em janelas subsequentes sem re-treinar.\"\"\"\n",
    "    train = df[df[cutoff_col] <= first_train_end].copy()\n",
    "    test  = df[(df[cutoff_col] >= test_start) & (df[cutoff_col] <= test_end)].copy()\n",
    "    return [(train, test)]  # único par\n",
    "\n",
    "def split_rolling(df, cutoff_col, first_train_end, horizon_stride, step_k=1) -> List[Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Expanding window: a cada iteração, amplia o treino e prevê o próximo bloco (stride).\n",
    "    step_k controla de quantas em quantas iterações re-treinamos (k=1 re-treina sempre).\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    current_end = first_train_end\n",
    "    i = 0\n",
    "    while True:\n",
    "        val_start = current_end + horizon_stride[0]\n",
    "        val_end   = current_end + horizon_stride[1]\n",
    "        train = df[df[cutoff_col] <= current_end].copy()\n",
    "        valid = df[(df[cutoff_col] > current_end) & (df[cutoff_col] <= val_end)].copy()\n",
    "        if valid.empty:\n",
    "            break\n",
    "        pairs.append((train, valid))\n",
    "        current_end = val_end\n",
    "        i += 1\n",
    "    return pairs\n",
    "\n",
    "# ---------- 2) Conjuntos de features ----------\n",
    "def feature_set_basic(cols_all: List[str]) -> List[str]:\n",
    "    \"\"\"Somente 'sales_qty_...' e calendário.\"\"\"\n",
    "    return [c for c in cols_all if c.startswith(\"sales_qty_\") or c.startswith(\"cal_\")]\n",
    "\n",
    "def feature_set_all(cols_all: List[str]) -> List[str]:\n",
    "    \"\"\"Todas as features criadas (exceto alvo/IDs).\"\"\"\n",
    "    return [c for c in cols_all if c not in (\"sales_qty\", \"id\", \"order_week\", \"have_nulls\", \"product_category_name\")]\n",
    "\n",
    "def feature_set_selected(selected_list_path: str) -> List[str]:\n",
    "    \"\"\"Carrega a lista congelada de relevância univariada.\"\"\"\n",
    "    with open(selected_list_path) as f:\n",
    "        feats = [ln.strip() for ln in f if ln.strip()]\n",
    "    return feats\n",
    "\n",
    "# ---------- 3) Modelos (LGBM com HPO opcional) ----------\n",
    "def _get_lgbm_search_objects(cfg_hpo: Dict):\n",
    "    \"\"\"Retorna (estimator, searcher) conforme config de HPO.\"\"\"\n",
    "    from lightgbm import LGBMRegressor\n",
    "    from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "    base = LGBMRegressor(\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    default_grid = {\n",
    "        \"num_leaves\": [31, 63, 127],\n",
    "        \"max_depth\": [-1, 8, 12],\n",
    "        \"learning_rate\": [0.03, 0.05, 0.1],\n",
    "        \"n_estimators\": [100, 400, 800],\n",
    "        \"subsample\": [0.8, 1.0],\n",
    "        \"colsample_bytree\": [0.8, 1.0],\n",
    "        \"min_child_samples\": [10, 20, 30],\n",
    "        \"reg_alpha\": [0.0, 0.1],\n",
    "        \"reg_lambda\": [0.0, 0.1]\n",
    "    }\n",
    "\n",
    "    search_kind = cfg_hpo.get(\"search\", \"random\")  # \"random\" | \"grid\"\n",
    "    scoring     = cfg_hpo.get(\"scoring\", \"neg_root_mean_squared_error\")\n",
    "    cv          = cfg_hpo.get(\"cv\", None)\n",
    "    n_jobs      = cfg_hpo.get(\"n_jobs\", -1)\n",
    "    verbose     = cfg_hpo.get(\"verbose\", 0)\n",
    "    param_grid  = cfg_hpo.get(\"param_grid\", default_grid)\n",
    "\n",
    "    if search_kind == \"grid\":\n",
    "        searcher = GridSearchCV(\n",
    "            estimator=base,\n",
    "            param_grid=param_grid,\n",
    "            scoring=scoring,\n",
    "            cv=cv,\n",
    "            n_jobs=n_jobs,\n",
    "            verbose=verbose\n",
    "        )\n",
    "    else:\n",
    "        n_iter = cfg_hpo.get(\"n_iter\", 30)\n",
    "        searcher = RandomizedSearchCV(\n",
    "            estimator=base,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=n_iter,\n",
    "            scoring=scoring,\n",
    "            cv=cv,\n",
    "            n_jobs=n_jobs,\n",
    "            verbose=verbose,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    return base, searcher\n",
    "\n",
    "def _make_timeseries_cv(train_df: pd.DataFrame, date_col: str, n_splits: int = 3):\n",
    "    \"\"\"Cria um TimeSeriesSplit consistente (ordenando por data).\"\"\"\n",
    "    from sklearn.model_selection import TimeSeriesSplit\n",
    "    return TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "def fit_predict_lgbm(\n",
    "    train: pd.DataFrame,\n",
    "    valid: pd.DataFrame,\n",
    "    features: List[str],\n",
    "    id_col: Optional[str],\n",
    "    date_col: str,\n",
    "    target_col: str,\n",
    "    cfg: Optional[Dict] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Treina e prediz com LGBM.\n",
    "    - Se cfg[\"lgbm_hpo\"][\"enable\"] for True, faz HPO temporal em train e depois ajusta best_estimator_ no train completo.\n",
    "    - Caso contrário, usa um conjunto fixo de hiperparâmetros.\n",
    "    Retorna (preds_valid, model).\n",
    "    \"\"\"\n",
    "    import lightgbm as lgb\n",
    "\n",
    "    cfg = cfg or {}\n",
    "    hpo_cfg = cfg.get(\"lgbm_hpo\", {\"enable\": False})\n",
    "\n",
    "    train_sorted = train.sort_values(date_col)\n",
    "    valid_sorted = valid.sort_values(date_col)\n",
    "\n",
    "    X_tr = train_sorted[features]\n",
    "    y_tr = train_sorted[target_col]\n",
    "    X_va = valid_sorted[features]\n",
    "\n",
    "    if hpo_cfg.get(\"enable\", False):\n",
    "        cv = _make_timeseries_cv(train_sorted, date_col, n_splits=hpo_cfg.get(\"cv_splits\", 3))\n",
    "        _, searcher = _get_lgbm_search_objects({\n",
    "            **hpo_cfg,\n",
    "            \"cv\": cv\n",
    "        })\n",
    "\n",
    "        searcher.fit(X_tr, y_tr)\n",
    "        best_params = searcher.best_params_\n",
    "\n",
    "        mdl = lgb.LGBMRegressor(random_state=42, **best_params)\n",
    "        mdl.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, valid_sorted[target_col])],\n",
    "            eval_metric=\"rmse\",\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=hpo_cfg.get(\"early_stopping_rounds\", 100), verbose=False)]\n",
    "        )\n",
    "    else:\n",
    "        mdl = lgb.LGBMRegressor(\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        mdl.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, valid_sorted[target_col])],\n",
    "            eval_metric=\"rmse\",\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]\n",
    "        )\n",
    "\n",
    "    preds = mdl.predict(X_va)\n",
    "    return preds, mdl\n",
    "\n",
    "# ---------- Prophet ----------\n",
    "def fit_predict_prophet(train, valid, features_exog, date_col, target_col):\n",
    "    \"\"\"\n",
    "    Assume que train/valid são de UMA série (ex.: 1 categoria).\n",
    "    Usa features_exog como regressores adicionais.\n",
    "    \"\"\"\n",
    "    from prophet import Prophet\n",
    "    df_p = train[[date_col, target_col] + features_exog].rename(columns={date_col: \"ds\", target_col: \"y\"})\n",
    "    m = Prophet()\n",
    "    for c in features_exog:\n",
    "        m.add_regressor(c)\n",
    "    m.fit(df_p)\n",
    "    df_future = valid[[date_col] + features_exog].rename(columns={date_col: \"ds\"})\n",
    "    yhat = m.predict(df_future)[\"yhat\"].values\n",
    "    return yhat, m\n",
    "\n",
    "# ---------- SARIMA ----------\n",
    "def fit_predict_sarima(\n",
    "    train: pd.DataFrame,\n",
    "    valid: pd.DataFrame,\n",
    "    date_col: str,\n",
    "    target_col: str,\n",
    "    cfg: Optional[Dict] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    SARIMA univariado (assume que train/valid contêm uma única série,\n",
    "    ex.: já filtrada por categoria).\n",
    "    \"\"\"\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "    cfg = cfg or {}\n",
    "    sar_cfg = cfg.get(\"sarima\", {})\n",
    "\n",
    "    order = sar_cfg.get(\"order\", (1, 1, 1))\n",
    "    seasonal_order = sar_cfg.get(\"seasonal_order\", (1, 1, 1, 52))  # semanal por padrão\n",
    "    enforce_stationarity = sar_cfg.get(\"enforce_stationarity\", True)\n",
    "    enforce_invertibility = sar_cfg.get(\"enforce_invertibility\", True)\n",
    "\n",
    "    train_sorted = train.sort_values(date_col)\n",
    "    valid_sorted = valid.sort_values(date_col)\n",
    "\n",
    "    y_tr = train_sorted[target_col].values\n",
    "\n",
    "    model = SARIMAX(\n",
    "        y_tr,\n",
    "        order=order,\n",
    "        seasonal_order=seasonal_order,\n",
    "        enforce_stationarity=enforce_stationarity,\n",
    "        enforce_invertibility=enforce_invertibility\n",
    "    )\n",
    "    res = model.fit(disp=False)\n",
    "\n",
    "    n_forecast = len(valid_sorted)\n",
    "    yhat = res.forecast(steps=n_forecast)\n",
    "    return np.asarray(yhat), res\n",
    "\n",
    "# ---------- LSTM ----------\n",
    "def _make_lstm_sequences(series: np.ndarray, lookback: int):\n",
    "    \"\"\"\n",
    "    Transforma série 1D em (X, y) para LSTM:\n",
    "    X: (n_samples, lookback, 1), y: (n_samples,)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(lookback, len(series)):\n",
    "        X.append(series[i - lookback:i])\n",
    "        y.append(series[i])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    # adiciona dimensão de feature = 1\n",
    "    X = X[..., None]\n",
    "    return X, y\n",
    "\n",
    "def fit_predict_lstm(\n",
    "    train: pd.DataFrame,\n",
    "    valid: pd.DataFrame,\n",
    "    date_col: str,\n",
    "    target_col: str,\n",
    "    cfg: Optional[Dict] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    LSTM univariado simples.\n",
    "    Assumimos uma única série (ex.: 1 categoria) em train/valid.\n",
    "    \"\"\"\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "    cfg = cfg or {}\n",
    "    lstm_cfg = cfg.get(\"lstm\", {})\n",
    "\n",
    "    lookback   = lstm_cfg.get(\"lookback\", 8)   # janelas de 8 semanas por padrão\n",
    "    epochs     = lstm_cfg.get(\"epochs\", 50)\n",
    "    batch_size = lstm_cfg.get(\"batch_size\", 16)\n",
    "    units      = lstm_cfg.get(\"units\", 32)\n",
    "    lr         = lstm_cfg.get(\"lr\", 1e-3)\n",
    "\n",
    "    train_sorted = train.sort_values(date_col)\n",
    "    valid_sorted = valid.sort_values(date_col)\n",
    "\n",
    "    y_tr = train_sorted[target_col].values.astype(\"float32\")\n",
    "\n",
    "    # Garante comprimento mínimo\n",
    "    if len(y_tr) <= lookback:\n",
    "        raise ValueError(f\"Série de treino muito curta para LSTM (len={len(y_tr)}, lookback={lookback}).\")\n",
    "\n",
    "    X_tr, y_tr_supervised = _make_lstm_sequences(y_tr, lookback)\n",
    "\n",
    "    model = Sequential([\n",
    "        LSTM(units, input_shape=(lookback, 1)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(loss=\"mse\", optimizer=Adam(learning_rate=lr))\n",
    "\n",
    "    model.fit(\n",
    "        X_tr, y_tr_supervised,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Forecast recursivo para o horizonte de validação\n",
    "    n_forecast = len(valid_sorted)\n",
    "    history = y_tr.copy()\n",
    "    preds = []\n",
    "\n",
    "    for _ in range(n_forecast):\n",
    "        if len(history) < lookback:\n",
    "            raise ValueError(\"Histórico insuficiente durante a fase de previsão LSTM.\")\n",
    "        x_input = history[-lookback:]\n",
    "        x_input = x_input.reshape(1, lookback, 1)\n",
    "        yhat = model.predict(x_input, verbose=0)[0, 0]\n",
    "        preds.append(float(yhat))\n",
    "        history = np.append(history, yhat)\n",
    "\n",
    "    return np.array(preds, dtype=\"float32\"), model\n",
    "\n",
    "# ---------- 4) Ensemble ----------\n",
    "def ensemble_mean(preds_dict: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"Média simples entre modelos.\"\"\"\n",
    "    stacked = np.vstack([v for v in preds_dict.values()])\n",
    "    return stacked.mean(axis=0)\n",
    "\n",
    "def ensemble_weighted(preds_dict: Dict[str, np.ndarray], weights: Dict[str, float]) -> np.ndarray:\n",
    "    keys = list(preds_dict.keys())\n",
    "    W = np.array([weights.get(k, 1.0) for k in keys])\n",
    "    W = W / W.sum()\n",
    "    stacked = np.vstack([preds_dict[k] for k in keys])\n",
    "    return (stacked * W[:, None]).sum(axis=0)\n",
    "\n",
    "# ---------- 5) Métricas ----------\n",
    "def wape(y_true, y_pred):\n",
    "    den = np.abs(y_true).sum()\n",
    "    return np.inf if den == 0 else np.abs(y_true - y_pred).sum() / den\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "# ---------- 6) Runner de cenário ----------\n",
    "def run_scenario(pairs, feature_mode, model_mode, cols_all, cfg) -> Dict:\n",
    "    \"\"\"\n",
    "    pairs: lista de (train, valid)\n",
    "    feature_mode: \"basic\" | \"all\" | \"selected\"\n",
    "    model_mode:\n",
    "        \"single:lgbm\"\n",
    "        \"single:prophet\"\n",
    "        \"single:sarima\"\n",
    "        \"single:lstm\"\n",
    "        \"ensemble:all\" (lgbm + prophet + sarima + lstm, quando disponíveis)\n",
    "    \"\"\"\n",
    "    # 6.1 escolhe features\n",
    "    if feature_mode == \"basic\":\n",
    "        feats = feature_set_basic(cols_all)\n",
    "    elif feature_mode == \"selected\":\n",
    "        feats = feature_set_selected(cfg[\"selected_list_path\"])\n",
    "    else:\n",
    "        feats = feature_set_all(cols_all)\n",
    "\n",
    "    results = []\n",
    "    for i, (tr, va) in enumerate(pairs):\n",
    "        y_true = va[cfg[\"target_col\"]].values\n",
    "        preds_pack = {}\n",
    "\n",
    "        # LGBM (usa features tabulares)\n",
    "        if model_mode in (\"single:lgbm\", \"ensemble:all\"):\n",
    "            p_lgbm, _ = fit_predict_lgbm(\n",
    "                tr, va, feats,\n",
    "                cfg.get(\"id_col\"), cfg[\"date_col\"], cfg[\"target_col\"],\n",
    "                cfg=cfg\n",
    "            )\n",
    "            preds_pack[\"lgbm\"] = p_lgbm\n",
    "\n",
    "        # Prophet (usa exógenas, aqui tudo que não começa com sales_qty_)\n",
    "        if model_mode in (\"single:prophet\", \"ensemble:all\"):\n",
    "            exog = [c for c in feats if not c.startswith(\"sales_qty_\")]\n",
    "            p_prophet, _ = fit_predict_prophet(tr, va, exog, cfg[\"date_col\"], cfg[\"target_col\"])\n",
    "            preds_pack[\"prophet\"] = p_prophet\n",
    "\n",
    "        # SARIMA (univariado)\n",
    "        if model_mode in (\"single:sarima\", \"ensemble:all\"):\n",
    "            p_sarima, _ = fit_predict_sarima(\n",
    "                tr, va,\n",
    "                date_col=cfg[\"date_col\"],\n",
    "                target_col=cfg[\"target_col\"],\n",
    "                cfg=cfg\n",
    "            )\n",
    "            preds_pack[\"sarima\"] = p_sarima\n",
    "\n",
    "        # LSTM (univariado)\n",
    "        if model_mode in (\"single:lstm\", \"ensemble:all\"):\n",
    "            p_lstm, _ = fit_predict_lstm(\n",
    "                tr, va,\n",
    "                date_col=cfg[\"date_col\"],\n",
    "                target_col=cfg[\"target_col\"],\n",
    "                cfg=cfg\n",
    "            )\n",
    "            preds_pack[\"lstm\"] = p_lstm\n",
    "\n",
    "        if model_mode.startswith(\"single:\"):\n",
    "            key = model_mode.split(\":\")[1]\n",
    "            y_pred = preds_pack[key]\n",
    "        else:\n",
    "            y_pred = ensemble_mean(preds_pack)\n",
    "\n",
    "        res = {\n",
    "            \"fold\": i,\n",
    "            \"feature_mode\": feature_mode,\n",
    "            \"model_mode\": model_mode,\n",
    "            \"WAPE\": wape(y_true, y_pred),\n",
    "            \"RMSE\": rmse(y_true, y_pred),\n",
    "        }\n",
    "        results.append(res)\n",
    "\n",
    "    return {\n",
    "        \"rows\": results,\n",
    "        \"avg_WAPE\": np.mean([r[\"WAPE\"] for r in results]),\n",
    "        \"avg_RMSE\": np.mean([r[\"RMSE\"] for r in results])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHmBzxcm-7w9"
   },
   "source": [
    "## lista de festures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4T9i8qqN90k4"
   },
   "outputs": [],
   "source": [
    "desconsiderar = [ 'approval_time_hours_mean_co',\n",
    " 'approval_time_hours_mean_ne',\n",
    " 'approval_time_hours_mean_n',\n",
    " 'approval_time_hours_mean_se',\n",
    " 'approval_time_hours_mean_s',\n",
    " 'delivery_diff_estimated_mean_co',\n",
    " 'delivery_diff_estimated_mean_ne',\n",
    " 'delivery_diff_estimated_mean_n',\n",
    " 'delivery_diff_estimated_mean_se',\n",
    " 'delivery_diff_estimated_mean_s',\n",
    " 'est_delivery_lead_days_mean_co',\n",
    " 'est_delivery_lead_days_mean_ne',\n",
    " 'est_delivery_lead_days_mean_n',\n",
    " 'est_delivery_lead_days_mean_se',\n",
    " 'est_delivery_lead_days_mean_s',\n",
    " 'delivery_diff_estimated_weighted',\n",
    " 'est_delivery_lead_days_weighted',\n",
    " 'approval_time_hours_weighted',\n",
    " 'customer_regions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlYDmHVRkFuB"
   },
   "outputs": [],
   "source": [
    "# Garante tipo datetime\n",
    "df[cutoff_col] = pd.to_datetime(df[cutoff_col])\n",
    "\n",
    "# Seleciona features \"completas\" (a função já exclui y/id/date por padrão)\n",
    "features = feature_set_all(df.columns.tolist())\n",
    "features = [c for c in features if c not in desconsiderar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smbR0GhAv3-L"
   },
   "outputs": [],
   "source": [
    "selected_features = ['sales_qty_roll8_mean',\n",
    " 'sales_qty_lag1',\n",
    " 'sales_qty_roll4_mean',\n",
    " 'sales_qty_lag2',\n",
    " 'sales_qty_lag4',\n",
    " 'sales_qty_lag8',\n",
    " 'sales_qty_roll8_std',\n",
    " 'sales_qty_roll4_std',\n",
    " 'approval_time_hours_weighted_roll8_std',\n",
    " 'price_var_m4_vs_prev4_mean_roll8_std',\n",
    " 'est_delivery_lead_days_weighted_roll8_std',\n",
    " 'approval_time_hours_weighted_roll4_std',\n",
    " 'est_delivery_lead_days_weighted',\n",
    " 'delivery_diff_estimated_weighted',\n",
    " 'est_delivery_lead_days_weighted_roll4_std',\n",
    " 'price_var_m4_vs_prev4_mean_roll4_std',\n",
    " 'price_var_w1_point_mean_roll8_std',\n",
    " 'price_var_w1_point_mean_roll4_std',\n",
    " 'price_var_w1_smooth_mean_roll8_std',\n",
    " 'approval_time_hours_weighted']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UEwKxz4_Dow"
   },
   "source": [
    "### teste com retreino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 82222,
     "status": "ok",
     "timestamp": 1763126796003,
     "user": {
      "displayName": "Eduardo Medeiros",
      "userId": "10594247742830776987"
     },
     "user_tz": 180
    },
    "id": "HuK0_Yj6qMON",
    "outputId": "f603de05-fc93-4844-bea6-cf5e13294514"
   },
   "outputs": [],
   "source": [
    "# Gera janelas rolling\n",
    "pairs_rolling = split_rolling(\n",
    "    df=df,\n",
    "    cutoff_col=cutoff_col,\n",
    "    first_train_end=first_train_end,\n",
    "    horizon_stride=horizon_stride,\n",
    "    step_k=1,  # re-treina a cada janela\n",
    ")\n",
    "\n",
    "preds_roll = []\n",
    "for tr_i, va_i in pairs_rolling:\n",
    "    # opcional: limitar a janela de validação ao período de teste\n",
    "    va_i = va_i[(va_i[cutoff_col] >= test_start) & (va_i[cutoff_col] <= test_end)]\n",
    "    if va_i.empty:\n",
    "        continue\n",
    "\n",
    "    yhat_i, _ = fit_predict_lgbm(\n",
    "        tr_i, va_i,\n",
    "        features=features,\n",
    "        id_col=id_col, date_col=date_col, target_col=target_col\n",
    "    )\n",
    "\n",
    "    tmp = va_i[[date_col, target_col] + ([id_col] if id_col in va_i.columns else [])].copy()\n",
    "    tmp[\"y_pred\"] = yhat_i\n",
    "    tmp.rename(columns={target_col: \"y_true\"}, inplace=True)\n",
    "    preds_roll.append(tmp)\n",
    "\n",
    "# Concatena previsões de todas as janelas (dentro do período de teste)\n",
    "df_pred_roll = pd.concat(preds_roll, ignore_index=True) if preds_roll else \\\n",
    "               pd.DataFrame(columns=[date_col, \"y_true\", \"y_pred\"])\n",
    "\n",
    "# Métricas\n",
    "m_roll = calculate_metrics(df_pred_roll, y_true=\"y_true\", y_pred=\"y_pred\")\n",
    "m_roll.to_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t48YtPYgpEFm"
   },
   "source": [
    "## teste 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Etoa6NjypFN6"
   },
   "outputs": [],
   "source": [
    "# Listas para acumular previsões e métricas por categoria\n",
    "all_preds_static = []\n",
    "all_preds_roll   = []\n",
    "metrics_static_list = []\n",
    "metrics_roll_list   = []\n",
    "\n",
    "# Itera por categoria de produto\n",
    "for cat, df_cat in df.groupby(\"product_category_name\"):\n",
    "    print(f\"Treinando categoria: {cat}\")\n",
    "\n",
    "    # ---------- TREINO ESTÁTICO (por categoria) ----------\n",
    "    pairs_static = split_static(\n",
    "        df=df_cat,\n",
    "        cutoff_col=cutoff_col,\n",
    "        first_train_end=first_train_end,\n",
    "        test_start=test_start,\n",
    "        test_end=test_end,\n",
    "    )\n",
    "\n",
    "    (train_static, test_static) = pairs_static[0]\n",
    "\n",
    "    # Se não houver dados de teste nessa categoria, pula\n",
    "    if test_static.empty:\n",
    "        print(f\"  >> Sem dados de teste para categoria {cat}, pulando treino estático.\")\n",
    "    else:\n",
    "        # Treina 1x no treino estático e prediz o teste\n",
    "        yhat_static, mdl_static = fit_predict_lgbm(\n",
    "            train_static, test_static,\n",
    "            features=features,\n",
    "            id_col=id_col, date_col=date_col, target_col=target_col\n",
    "        )\n",
    "\n",
    "        # Monta DF de avaliação padronizado\n",
    "        base_cols = [date_col, target_col, \"product_category_name\"]\n",
    "        if id_col in test_static.columns:\n",
    "            base_cols.append(id_col)\n",
    "\n",
    "        df_pred_static = test_static[base_cols].copy()\n",
    "        df_pred_static[\"y_pred\"] = yhat_static\n",
    "        df_pred_static.rename(columns={target_col: \"y_true\"}, inplace=True)\n",
    "\n",
    "        # Métricas por categoria\n",
    "        m_static = calculate_metrics(df_pred_static, y_true=\"y_true\", y_pred=\"y_pred\")\n",
    "        # Guarda as métricas em forma de dict, adicionando a categoria\n",
    "        metrics_static_list.append({\n",
    "            \"product_category_name\": cat,\n",
    "            **m_static.to_dict()\n",
    "        })\n",
    "\n",
    "        # Guarda predições para concatenar depois\n",
    "        all_preds_static.append(df_pred_static)\n",
    "\n",
    "    # ---------- TREINO COM JANELAS (por categoria) ----------\n",
    "    pairs_rolling = split_rolling(\n",
    "        df=df_cat,\n",
    "        cutoff_col=cutoff_col,\n",
    "        first_train_end=first_train_end,\n",
    "        horizon_stride=horizon_stride,\n",
    "        step_k=1,  # re-treina a cada janela\n",
    "    )\n",
    "\n",
    "    preds_roll_cat = []\n",
    "    for tr_i, va_i in pairs_rolling:\n",
    "        # Limita a janela de validação ao período de teste global\n",
    "        va_i = va_i[\n",
    "            (va_i[cutoff_col] >= test_start) &\n",
    "            (va_i[cutoff_col] <= test_end)\n",
    "        ]\n",
    "        if va_i.empty:\n",
    "            continue\n",
    "\n",
    "        yhat_i, _ = fit_predict_lgbm(\n",
    "            tr_i, va_i,\n",
    "            features=features,\n",
    "            id_col=id_col, date_col=date_col, target_col=target_col\n",
    "        )\n",
    "\n",
    "        base_cols = [date_col, target_col, \"product_category_name\"]\n",
    "        if id_col in va_i.columns:\n",
    "            base_cols.append(id_col)\n",
    "\n",
    "        tmp = va_i[base_cols].copy()\n",
    "        tmp[\"y_pred\"] = yhat_i\n",
    "        tmp.rename(columns={target_col: \"y_true\"}, inplace=True)\n",
    "        preds_roll_cat.append(tmp)\n",
    "\n",
    "    # Se houve ao menos uma janela válida, calcula métricas para a categoria\n",
    "    if preds_roll_cat:\n",
    "        df_pred_roll_cat = pd.concat(preds_roll_cat, ignore_index=True)\n",
    "        m_roll = calculate_metrics(df_pred_roll_cat, y_true=\"y_true\", y_pred=\"y_pred\")\n",
    "        metrics_roll_list.append({\n",
    "            \"product_category_name\": cat,\n",
    "            **m_roll.to_dict()\n",
    "        })\n",
    "\n",
    "        all_preds_roll.append(df_pred_roll_cat)\n",
    "    else:\n",
    "        print(f\"  >> Sem janelas válidas no período de teste para categoria {cat}, pulando treino rolling.\")\n",
    "\n",
    "# ---------- Consolida resultados de TODAS as categorias ----------\n",
    "\n",
    "# DataFrames de predições consolidadas\n",
    "df_pred_static_all = (\n",
    "    pd.concat(all_preds_static, ignore_index=True)\n",
    "    if all_preds_static else\n",
    "    pd.DataFrame(columns=[date_col, \"y_true\", \"y_pred\", \"product_category_name\"] + ([id_col] if id_col in df.columns else []))\n",
    ")\n",
    "\n",
    "df_pred_roll_all = (\n",
    "    pd.concat(all_preds_roll, ignore_index=True)\n",
    "    if all_preds_roll else\n",
    "    pd.DataFrame(columns=[date_col, \"y_true\", \"y_pred\", \"product_category_name\"] + ([id_col] if id_col in df.columns else []))\n",
    ")\n",
    "\n",
    "# Métricas por categoria (1 linha por categoria)\n",
    "metrics_static_df = pd.DataFrame(metrics_static_list)  # treino estático\n",
    "metrics_roll_df   = pd.DataFrame(metrics_roll_list)    # treino com janelas\n",
    "\n",
    "# Se quiser um dict {categoria: métricas}\n",
    "metrics_static_by_cat = {\n",
    "    row[\"product_category_name\"]: row.drop(\"product_category_name\").to_dict()\n",
    "    for _, row in metrics_static_df.iterrows()\n",
    "}\n",
    "\n",
    "metrics_roll_by_cat = {\n",
    "    row[\"product_category_name\"]: row.drop(\"product_category_name\").to_dict()\n",
    "    for _, row in metrics_roll_df.iterrows()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1763129272141,
     "user": {
      "displayName": "Eduardo Medeiros",
      "userId": "10594247742830776987"
     },
     "user_tz": 180
    },
    "id": "k8M-yBkIsHDt",
    "outputId": "e3c01f60-13c2-46d4-846d-0899fb53fa52"
   },
   "outputs": [],
   "source": [
    "resultado = pd.DataFrame(metrics_roll_by_cat)\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1763129315797,
     "user": {
      "displayName": "Eduardo Medeiros",
      "userId": "10594247742830776987"
     },
     "user_tz": 180
    },
    "id": "F-5hNNUksa2l",
    "outputId": "a46fe1b0-78a7-45b0-cc9d-f9de518440c6"
   },
   "outputs": [],
   "source": [
    "# calculando a média pelo index mape\n",
    "resultado.mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BW1p1_wXSjN"
   },
   "source": [
    "## teste 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "output_embedded_package_id": "1kNKCcGZy9pAx7zcRgEZxREYdW4CXaC3T"
    },
    "executionInfo": {
     "elapsed": 3174608,
     "status": "ok",
     "timestamp": 1763462552902,
     "user": {
      "displayName": "Eduardo Medeiros",
      "userId": "10594247742830776987"
     },
     "user_tz": 180
    },
    "id": "rGWXaqIiXT_G",
    "outputId": "f1186d1c-73c7-4070-b23a-654aa97fc2f8"
   },
   "outputs": [],
   "source": [
    "# Lista de modelos que vamos rodar\n",
    "model_names = [\"lgbm\", \"prophet\", \"sarima\", \"lstm\"]\n",
    "\n",
    "# Listas para acumular previsões e métricas por categoria\n",
    "all_preds_static = []  # cada df terá colunas: y_true, y_pred_lgbm, ..., y_pred_ensemble\n",
    "all_preds_roll   = []\n",
    "\n",
    "metrics_static_list = []  # uma linha por (categoria, modelo, esquema)\n",
    "metrics_roll_list   = []\n",
    "\n",
    "# Itera por categoria de produto\n",
    "for cat, df_cat in df.groupby(\"product_category_name\"):\n",
    "    print(f\"Treinando categoria: {cat}\")\n",
    "\n",
    "    # ---------- TREINO ESTÁTICO (por categoria) ----------\n",
    "    pairs_static = split_static(\n",
    "        df=df_cat,\n",
    "        cutoff_col=cutoff_col,\n",
    "        first_train_end=first_train_end,\n",
    "        test_start=test_start,\n",
    "        test_end=test_end,\n",
    "    )\n",
    "\n",
    "    (train_static, test_static) = pairs_static[0]\n",
    "\n",
    "    # Se não houver dados de teste nessa categoria, pula\n",
    "    if test_static.empty:\n",
    "        print(f\"  >> Sem dados de teste para categoria {cat}, pulando treino estático.\")\n",
    "    else:\n",
    "        # Base comum (sem previsões ainda)\n",
    "        base_cols = [date_col, target_col, \"product_category_name\"]\n",
    "        if id_col in test_static.columns:\n",
    "            base_cols.append(id_col)\n",
    "\n",
    "        df_pred_static = test_static[base_cols].copy()\n",
    "        df_pred_static.rename(columns={target_col: \"y_true\"}, inplace=True)\n",
    "\n",
    "        # Dicionário para guardar arrays de previsões por modelo\n",
    "        preds_static_dict = {}\n",
    "\n",
    "        # --- LGBM ---\n",
    "        try:\n",
    "            yhat_lgbm, mdl_static_lgbm = fit_predict_lgbm(\n",
    "                train_static, test_static,\n",
    "                features=features,\n",
    "                id_col=id_col, date_col=date_col, target_col=target_col\n",
    "            )\n",
    "            preds_static_dict[\"lgbm\"] = np.asarray(yhat_lgbm)\n",
    "        except Exception as e:\n",
    "            print(f\"  >> Erro ao treinar LGBM (static) categoria {cat}: {e}\")\n",
    "\n",
    "        # --- Prophet ---\n",
    "        try:\n",
    "            # mesmas exógenas que você usou no run_scenario: tudo que não começa com 'sales_qty_'\n",
    "            exog = [c for c in features if not c.startswith(\"sales_qty_\")]\n",
    "            yhat_prophet, mdl_static_prophet = fit_predict_prophet(\n",
    "                train_static, test_static,\n",
    "                features_exog=exog,\n",
    "                date_col=date_col,\n",
    "                target_col=target_col\n",
    "            )\n",
    "            preds_static_dict[\"prophet\"] = np.asarray(yhat_prophet)\n",
    "        except Exception as e:\n",
    "            print(f\"  >> Erro ao treinar Prophet (static) categoria {cat}: {e}\")\n",
    "\n",
    "        # --- SARIMA ---\n",
    "        try:\n",
    "            yhat_sarima, mdl_static_sarima = fit_predict_sarima(\n",
    "                train_static, test_static,\n",
    "                date_col=date_col,\n",
    "                target_col=target_col,\n",
    "                cfg=None  # ou passe um dict se tiver cfg[\"sarima\"]\n",
    "            )\n",
    "            preds_static_dict[\"sarima\"] = np.asarray(yhat_sarima)\n",
    "        except Exception as e:\n",
    "            print(f\"  >> Erro ao treinar SARIMA (static) categoria {cat}: {e}\")\n",
    "\n",
    "        # --- LSTM ---\n",
    "        try:\n",
    "            yhat_lstm, mdl_static_lstm = fit_predict_lstm(\n",
    "                train_static, test_static,\n",
    "                date_col=date_col,\n",
    "                target_col=target_col,\n",
    "                cfg=None  # ou cfg com parâmetros de LSTM\n",
    "            )\n",
    "            preds_static_dict[\"lstm\"] = np.asarray(yhat_lstm)\n",
    "        except Exception as e:\n",
    "            print(f\"  >> Erro ao treinar LSTM (static) categoria {cat}: {e}\")\n",
    "\n",
    "        # Garante que todos os arrays têm o mesmo tamanho da base\n",
    "        n_test = len(df_pred_static)\n",
    "        preds_static_dict = {\n",
    "            k: v for k, v in preds_static_dict.items()\n",
    "            if len(v) == n_test\n",
    "        }\n",
    "\n",
    "        if not preds_static_dict:\n",
    "            print(f\"  >> Nenhum modelo válido (static) para categoria {cat}.\")\n",
    "        else:\n",
    "            # Adiciona colunas de previsão por modelo\n",
    "            for mname, yhat in preds_static_dict.items():\n",
    "                df_pred_static[f\"y_pred_{mname}\"] = yhat\n",
    "\n",
    "            # Ensemble (média simples dos modelos disponíveis)\n",
    "            df_pred_static[\"y_pred_ensemble\"] = ensemble_mean(preds_static_dict)\n",
    "\n",
    "            # ---- Métricas por modelo (inclui ensemble) ----\n",
    "            modelos_para_metricas = list(preds_static_dict.keys()) + [\"ensemble\"]\n",
    "\n",
    "            for mname in modelos_para_metricas:\n",
    "                col_pred = f\"y_pred_{mname}\"\n",
    "                df_tmp = df_pred_static[[date_col, \"y_true\", col_pred, \"product_category_name\"]].copy()\n",
    "                df_tmp.rename(columns={col_pred: \"y_pred\"}, inplace=True)\n",
    "\n",
    "                m_static = calculate_metrics(df_tmp, y_true=\"y_true\", y_pred=\"y_pred\")\n",
    "                metrics_static_list.append({\n",
    "                    \"product_category_name\": cat,\n",
    "                    \"train_scheme\": \"static\",\n",
    "                    \"model\": mname,\n",
    "                    **m_static.to_dict()\n",
    "                })\n",
    "\n",
    "            # Guarda predições para concatenar depois\n",
    "            all_preds_static.append(df_pred_static)\n",
    "\n",
    "    # ---------- TREINO COM JANELAS (por categoria) ----------\n",
    "    pairs_rolling = split_rolling(\n",
    "        df=df_cat,\n",
    "        cutoff_col=cutoff_col,\n",
    "        first_train_end=first_train_end,\n",
    "        horizon_stride=horizon_stride,\n",
    "        step_k=1,  # re-treina a cada janela\n",
    "    )\n",
    "\n",
    "    preds_roll_cat = []  # lista de DFs com y_true + preds de cada modelo por janela\n",
    "\n",
    "    for tr_i, va_i in pairs_rolling:\n",
    "        # Limita a janela de validação ao período de teste global\n",
    "        va_i = va_i[\n",
    "            (va_i[cutoff_col] >= test_start) &\n",
    "            (va_i[cutoff_col] <= test_end)\n",
    "        ]\n",
    "        if va_i.empty:\n",
    "            continue\n",
    "\n",
    "        base_cols = [date_col, target_col, \"product_category_name\"]\n",
    "        if id_col in va_i.columns:\n",
    "            base_cols.append(id_col)\n",
    "\n",
    "        tmp = va_i[base_cols].copy()\n",
    "        tmp.rename(columns={target_col: \"y_true\"}, inplace=True)\n",
    "\n",
    "        preds_roll_dict = {}\n",
    "\n",
    "        # --- LGBM (rolling) ---\n",
    "        try:\n",
    "            yhat_i_lgbm, _ = fit_predict_lgbm(\n",
    "                tr_i, va_i,\n",
    "                features=features,\n",
    "                id_col=id_col, date_col=date_col, target_col=target_col\n",
    "            )\n",
    "            preds_roll_dict[\"lgbm\"] = np.asarray(yhat_i_lgbm)\n",
    "        except Exception as e:\n",
    "            print(f\"  >> Erro LGBM (rolling) categoria {cat}: {e}\")\n",
    "\n",
    "        # --- Prophet (rolling) ---\n",
    "        try:\n",
    "            exog = [c for c in features if not c.startswith(\"sales_qty_\")]\n",
    "            yhat_i_prophet, _ = fit_predict_prophet(\n",
    "                tr_i, va_i,\n",
    "                features_exog=exog,\n",
    "                date_col=date_col,\n",
    "                target_col=target_col\n",
    "            )\n",
    "            preds_roll_dict[\"prophet\"] = np.asarray(yhat_i_prophet)\n",
    "        except Exception as e:\n",
    "            print(f\"  >> Erro Prophet (rolling) categoria {cat}: {e}\")\n",
    "\n",
    "        # --- SARIMA (rolling) ---\n",
    "        try:\n",
    "            yhat_i_sarima, _ = fit_predict_sarima(\n",
    "                tr_i, va_i,\n",
    "                date_col=date_col,\n",
    "                target_col=target_col,\n",
    "                cfg=None\n",
    "            )\n",
    "            preds_roll_dict[\"sarima\"] = np.asarray(yhat_i_sarima)\n",
    "        except Exception as e:\n",
    "            print(f\"  >> Erro SARIMA (rolling) categoria {cat}: {e}\")\n",
    "\n",
    "        # --- LSTM (rolling) ---\n",
    "        try:\n",
    "            yhat_i_lstm, _ = fit_predict_lstm(\n",
    "                tr_i, va_i,\n",
    "                date_col=date_col,\n",
    "                target_col=target_col,\n",
    "                cfg=None\n",
    "            )\n",
    "            preds_roll_dict[\"lstm\"] = np.asarray(yhat_i_lstm)\n",
    "        except Exception as e:\n",
    "            print(f\"  >> Erro LSTM (rolling) categoria {cat}: {e}\")\n",
    "\n",
    "        n_va = len(tmp)\n",
    "        preds_roll_dict = {\n",
    "            k: v for k, v in preds_roll_dict.items()\n",
    "            if len(v) == n_va\n",
    "        }\n",
    "\n",
    "        if not preds_roll_dict:\n",
    "            continue\n",
    "\n",
    "        for mname, yhat in preds_roll_dict.items():\n",
    "            tmp[f\"y_pred_{mname}\"] = yhat\n",
    "\n",
    "        tmp[\"y_pred_ensemble\"] = ensemble_mean(preds_roll_dict)\n",
    "\n",
    "        preds_roll_cat.append(tmp)\n",
    "\n",
    "    # Se houve ao menos uma janela válida, calcula métricas para a categoria\n",
    "    if preds_roll_cat:\n",
    "        df_pred_roll_cat = pd.concat(preds_roll_cat, ignore_index=True)\n",
    "\n",
    "        modelos_para_metricas = [\n",
    "            c.replace(\"y_pred_\", \"\") for c in df_pred_roll_cat.columns\n",
    "            if c.startswith(\"y_pred_\")\n",
    "        ]\n",
    "\n",
    "        for mname in modelos_para_metricas:\n",
    "            col_pred = f\"y_pred_{mname}\"\n",
    "            df_tmp = df_pred_roll_cat[[date_col, \"y_true\", col_pred, \"product_category_name\"]].copy()\n",
    "            df_tmp.rename(columns={col_pred: \"y_pred\"}, inplace=True)\n",
    "\n",
    "            m_roll = calculate_metrics(df_tmp, y_true=\"y_true\", y_pred=\"y_pred\")\n",
    "            metrics_roll_list.append({\n",
    "                \"product_category_name\": cat,\n",
    "                \"train_scheme\": \"rolling\",\n",
    "                \"model\": mname,\n",
    "                **m_roll.to_dict()\n",
    "            })\n",
    "\n",
    "        all_preds_roll.append(df_pred_roll_cat)\n",
    "    else:\n",
    "        print(f\"  >> Sem janelas válidas no período de teste para categoria {cat}, pulando treino rolling.\")\n",
    "\n",
    "# ---------- Consolida resultados de TODAS as categorias ----------\n",
    "\n",
    "# DataFrames de predições consolidadas\n",
    "if all_preds_static:\n",
    "    df_pred_static_all = pd.concat(all_preds_static, ignore_index=True)\n",
    "else:\n",
    "    base_cols = [date_col, \"y_true\", \"product_category_name\"] + (\n",
    "        [id_col] if id_col in df.columns else []\n",
    "    )\n",
    "    df_pred_static_all = pd.DataFrame(columns=base_cols)\n",
    "\n",
    "if all_preds_roll:\n",
    "    df_pred_roll_all = pd.concat(all_preds_roll, ignore_index=True)\n",
    "else:\n",
    "    base_cols = [date_col, \"y_true\", \"product_category_name\"] + (\n",
    "        [id_col] if id_col in df.columns else []\n",
    "    )\n",
    "    df_pred_roll_all = pd.DataFrame(columns=base_cols)\n",
    "\n",
    "# Métricas por categoria / modelo / esquema\n",
    "metrics_static_df = pd.DataFrame(metrics_static_list)\n",
    "metrics_roll_df   = pd.DataFrame(metrics_roll_list)\n",
    "\n",
    "# Se quiser um dict { (categoria, modelo, esquema): métricas }\n",
    "metrics_static_by_key = {\n",
    "    (row[\"product_category_name\"], row[\"model\"], row[\"train_scheme\"]): row.drop(\n",
    "        [\"product_category_name\", \"model\", \"train_scheme\"]\n",
    "    ).to_dict()\n",
    "    for _, row in metrics_static_df.iterrows()\n",
    "}\n",
    "\n",
    "metrics_roll_by_key = {\n",
    "    (row[\"product_category_name\"], row[\"model\"], row[\"train_scheme\"]): row.drop(\n",
    "        [\"product_category_name\", \"model\", \"train_scheme\"]\n",
    "    ).to_dict()\n",
    "    for _, row in metrics_roll_df.iterrows()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1763463935339,
     "user": {
      "displayName": "Eduardo Medeiros",
      "userId": "10594247742830776987"
     },
     "user_tz": 180
    },
    "id": "wD7_Bmi3pCek",
    "outputId": "c050b450-a797-4e63-89d5-55dce80e2fcd"
   },
   "outputs": [],
   "source": [
    "metrics_roll_by_key"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOjNovD+TXmHG4792xSRZGh",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
